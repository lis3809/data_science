{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.sparse.linalg import svds\n",
    "import random\n",
    "\n",
    "pd.options.display.max_colwidth = 2000\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"border:3px #ff0000  solid;\">\n",
    "Научиться рекомендовать пользователям фильмы на основе факта просмотра фильмов пользователями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"border:3px #ff0000  solid;\">\n",
    "1. Для решения задачи будем использовать те же данные, которые были использованы в скринкастах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('C:/Users/Z/PycharmProjects/data_science_1t/task_2.7/movies.csv')\n",
    "rating_df = pd.read_csv('C:/Users/Z/PycharmProjects/data_science_1t/task_2.7/ratings_df_sample_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Для удобства отмасштабируем идентификаторы фильмов таким образом, \n",
    "# чтобы они начинались с 0 и заканчивались на n_movies-1\n",
    "movies_values = rating_df['movieId'].unique()\n",
    "\n",
    "rating_df['movieId'] = rating_df['movieId'].apply(lambda f: np.where(movies_values == f)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Также отмасштабируем идентификаторы пользователей таким образом, \n",
    "# чтобы они начинались с 0 и заканчивались на n_users-1\n",
    "users_values = rating_df['userId'].unique()\n",
    "\n",
    "rating_df['userId'] = rating_df['userId'].apply(lambda f: np.where(users_values == f)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"border:3px #ff0000  solid;\">\n",
    "2. Предположим, постановка рейтинга — обязательное по итогам просмотра фильмов действие. Основываясь на этом, сгенерируйте новый целевой признак «факт просмотра фильма пользователем», который будет равен 1 для всех пар пользователь * фильм из подгруженного датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df['is_viewed'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"border:3px #ff0000  solid;\">\n",
    "3. А откуда взять «нолики»? В наших данных есть только пары пользователь * фильм, в которых пользователь точно смотрел фильм. Но для обучения модели нужны так называемые «негативы», то есть, пары, где пользователь фильм не смотрел. На практике приходится сталкиваться с необходимостью генерировать их вручную, давайте потренируемся это делать. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Сначала найдите уникальные id всех пользователей и уникальные id всех фильмов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_id_uniq = rating_df['userId'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_id_uniq = rating_df['movieId'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    С помощью функции random.choice (документация) сгенерируйте случайные пары пользователь * фильм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Поскольку среди сгенерированных пар могут быть и такие, что пользователь в них уже смотрел фильм, сгенерируйте побольше пар, например, удвоенное количество строк из источника. Это может занять пару минут."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pars = []\n",
    "for i in range(6040099*2):\n",
    "    para = (random.choice(users_id_uniq), random.choice(movies_id_uniq))\n",
    "    random_pars.append(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pars_df = pd.DataFrame(random_pars)\n",
    "random_pars_df.columns = ['userId', 'movieId']\n",
    "random_pars_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Среди сгенерированных пар могут быть и дубликаты, удалите их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pars_df.drop_duplicates(inplace=True)\n",
    "random_pars_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Оставьте среди сгенерированных пар только те, в которых пользователь фильм не смотрел."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_pars = rating_df[['userId', 'movieId']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pars_df = set(tuple(x) for x in random_pars_df.values)\n",
    "origin_pars = set(tuple(x) for x in origin_pars.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pars_df_not_viewed = pd.DataFrame(random_pars_df.difference(origin_pars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pars_df_not_viewed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Возможно, пар получилось больше, чем нужно, выберите из них столько, сколько у нас строк в исходных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pars_df_not_viewed = random_pars_df_not_viewed.sample(6040099)\n",
    "random_pars_df_not_viewed.columns = ['userId', 'movieId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Добавьте очищенные сгенерированные пары к исходным данным. Значение целевого признака в них будет равно нулю. Убедитесь, что у вас не появились дубликаты в датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = pd.concat([rating_df, random_pars_df_not_viewed], ignore_index=True)\n",
    "rating_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df[['userId', 'movieId']].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.fillna(0, inplace=True)\n",
    "rating_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"border:3px #ff0000  solid;\">\n",
    "4. Подготовьте датасет к обучению: отделите тестовую часть от тренировочной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(rating_df, test_size=0.1, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train['is_viewed']\n",
    "train_features = train.drop(['is_viewed'], axis=1)\n",
    "test_target = test['is_viewed']\n",
    "test_features = test.drop(['is_viewed'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"border:3px #ff0000  solid;\">\n",
    "5. Обучите dummy-model. Пусть она будет возвращать случайную вероятность принадлежности классу 1. Для этого можете использовать функцию random.random (документация). Оцените ее качество какой-то метрикой на свой вкус. Необходимо прогнозировать именно вероятность, чтобы была возможность ранжировать по ней варианты для рекомендации лучшего контента пользователю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dum_model(features):\n",
    "    return [random.random() for i in range(len(features))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_predict = dum_model(test_features)\n",
    "roc_auc_dum = round((roc_auc_score(test_target, dum_predict)), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df_result = {'ROC-AUC':[roc_auc_dum]}\n",
    "rating_df_result = pd.DataFrame(rating_df_result)\n",
    "rating_df_result = rating_df_result.rename(index={0:'dummy_model'})\n",
    "rating_df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"border:3px #ff0000  solid;\">\n",
    "6. Реализуйте три алгоритма коллаборативной фильтрации: user-, item-based и алгоритм на основе матричной факторизации. Оцените их качество и адекватность. Если качество недостаточно хорошее, попробуйте варьировать параметры: количество похожих пользователей/фильмов, количество элементов в матрицах при матричном разложении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = train['userId'].nunique()\n",
    "n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_movies = train['movieId'].nunique()\n",
    "n_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_matrix = np.array(pd.pivot_table(train, values='is_viewed', index='userId', columns='movieId', fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"border:3px #00B344  solid;\">\n",
    "Рассчитаем попарное косинусное расстояние для пользователей и для фильмов. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_similarity = cosine_distances(train_data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_similarity = cosine_distances(train_data_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_similarity.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"border:3px #00B344  solid;\">\n",
    "Реализуем алгоритм user-based. Для каждого пользователя найдем топ 10 ближайших, исключая самого себя. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_user_similarity = []\n",
    "for i in range(n_users):\n",
    "    neighbors = (user_similarity[i]).argsort()[1:10+1]\n",
    "    top_user_similarity.append(\n",
    "        train_data_matrix[neighbors]\n",
    "    )\n",
    "top_user_similarity = np.array(top_user_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_user_similarity.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составим матрицу предсказаний, оценив популяронсть фильмов среди пользователей и их соседей относительно других фильмов, посчитав среднее значение просмотров в топ-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_movie_popular = top_user_similarity.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_movie_popular.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"border:3px #00B344  solid;\">\n",
    "Предсказание для тестового датасета \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_near(x):\n",
    "    return round(x*2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['predict_user_based'] = test.apply(\n",
    "    lambda f: round_to_near(predict_movie_popular[int(f['userId']), int(f['movieId'])]), axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.where(test['predict_user_based'] >= 0.5, 1, 0)\n",
    "roc_auc_user_based = round(roc_auc_score(prediction, test['is_viewed']), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df_result.loc[len(rating_df_result.index)] = [roc_auc_user_based]\n",
    "rating_df_result = rating_df_result.rename(index={1:'user_based'})\n",
    "rating_df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"border:3px #00B344  solid;\">\n",
    "Реализуем алгоритм item-based.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_movie_similarity = []\n",
    "for i in range(n_movies):\n",
    "    neighbors_m = (movie_similarity[i]).argsort()[1:10+1]\n",
    "    top_movie_similarity.append(\n",
    "        train_data_matrix.T[neighbors_m]\n",
    "    )\n",
    "top_movie_similarity = np.array(top_movie_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_movie_similarity.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого пользователя и каждого фильма считаем среднее значение популярности от пользователя для топ-10 соседних фильмов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_mov_viewed_item_based = top_movie_similarity.mean(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_mov_viewed_item_based.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем предсказание для тестового датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['predict_movie_based'] = test.apply(\n",
    "    lambda f: round_to_near(predict_mov_viewed_item_based[int(f['userId']), int(f['movieId'])]), axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.where(test['predict_movie_based'] >= 0.5, 1, 0)\n",
    "roc_auc_item_based = round(roc_auc_score(prediction, test['is_viewed']), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df_result.loc[len(rating_df_result.index)] = [roc_auc_item_based]\n",
    "rating_df_result = rating_df_result.rename(index={2:'item_based'})\n",
    "rating_df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"border:3px #00B344  solid;\">\n",
    "Реализуем алгоритм на основе матричной факторизации. Сделаем SVD.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_matrix = train_data_matrix.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vh = svds(train_data_matrix, k=40)\n",
    "s_diag_matrix = np.diag(s)\n",
    "\n",
    "users = np.dot(u, s_diag_matrix)\n",
    "items = vh.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(s).shape, users.shape, items.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['predict_svds'] = test.apply(\n",
    "    lambda f: round_to_near(np.dot(users[int(f['userId'])], items[int(f['movieId'])])), axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.where(test['predict_svds'] >= 0.5, 1, 0)\n",
    "roc_auc_svd = round(roc_auc_score(prediction, test['is_viewed']), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df_result.loc[len(rating_df_result.index)] = [roc_auc_svd]\n",
    "rating_df_result = rating_df_result.rename(index={3:'svd'})\n",
    "rating_df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"border:3px #ff0000  solid;\">\n",
    "7. Опишите вывод, содержащий информацию о том, какой алгоритм проявил себя лучше всего."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"border:3px #00B344  solid;\">\n",
    "Вывод: Исходя из полученных значений метрики ROC-AUC для каждой модели, можно сделать вывод, что наилучшим качеством обладает модель на основе матричной факторизации.\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
